# Multimodality with Gemini

## Overview

This notebook demonstrates how to use **Gemini’s multimodal capabilities** within **Vertex AI** to work with both **images and text** at the same time. This is especially useful when combining visual and textual data leads to more meaningful results.

## What You’ll Learn

* Set up and configure **Vertex AI** in your own environment.
* Load and use the **Gemini multimodal model** via Python.
* Send a combination of **text and image** as input to the model.
* Interpret the **AI-generated output** based on that input.

## Tasks

1. Set up **Vertex AI** in your Google Cloud project.
2. Load the multimodal model: `gemini-1.0-pro-vision`.
3. Test it with an example such as:

   * **Image**: A picture of food (e.g., scones)
   * **Prompt**: “What do you see in this image?”
4. Display and analyze the model’s response.

## Learning Outcomes

By the end of this lab, you will be able to:

* Use AI models that process both images and text together.
* Apply **multimodal AI** for more context-aware outputs.
* Build a foundation for projects like **document understanding**, **product recognition**, and other **Generative AI applications**.